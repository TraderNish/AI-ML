
@author Nishant

# Import necessary modules

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sns
from sklearn.metrics import r2_score

"""
Load 1 year dtaa where S_Score > 2.0 and S_Delta > 2.0 for various frequencies and try to find out which independent variable has related with the
response variable

Independent variables = [['raw_s', 's_delta', 's_score']] -- We have just mentioned couple of independent variables. In actual, you need to include all 
                                                              the independent variables

Response variable = [return_at_t_plus10]

"""

print("Current Working Directory ", os.getcwd())

df = pd.read_csv('S_Score And S_Delta Greater Than 2.csv')

# Create arrays for features and target variables

y = df['return_at_t_plus10']

x = df[['raw_s', 's_delta', 's_score']]

"""
  We usually split the data into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized
  to other data later on.
  
  We have the test dataset (or subset) in order to test our model's prediction on this subset. 
  
  We will do this using the scikit-learn library and specifically the train_test_split method

"""

# Create training and testing sets
# we will use common split 70-30
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

"""
Now we will use the Gradient Boosting algorithm. As a first step, we will define a Gradient Boosting regressor and fit it to the training set. 

The dataset is ready and split into 80% train and 20% test. The features matrix X_train and the array y_train are available. 

Import GradientBoostingRegressor from sklearn.ensemble. Instantiate a gradient boosting regressor by setting the parameters:

max_depth to 4
n_estimators to 100

"""
# import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor

gb = GradientBoostingRegressor(
      n_estimators = 100,
      max_depth = 4,
      max_features = 'Log2',
      min_samples_leaf = 10,
      random_state = 2)
      
      









