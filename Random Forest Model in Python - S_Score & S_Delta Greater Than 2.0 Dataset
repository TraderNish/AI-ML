
@author Nishant

# Import necessary modules

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sns

"""
Load 1 year dtaa where S_Score > 2.0 and S_Delta > 2.0 for various frequencies and try to find out which independent variable has related with the
response variable

Independent variables = [['raw_s', 's_delta', 's_score']] -- We have just mentioned couple of independent variables. In actual, you need to include all 
                                                              the independent variables

Response variable = [return_at_t_plus10]

"""

print("Current Working Directory ", os.getcwd())

df = pd.read_csv('S_Score And S_Delta Greater Than 2.csv')

# Create arrays for features and target variables

y = df['return_at_t_plus10'].values

x = [['raw_s', 's_delta', 's_score']].values

"""
  We usually split the data into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized
  to other data later on.
  
  We have the test dataset (or subset) in order to test our model's prediction on this subset. 
  
  We will do this using the scikit-learn library and specifically the train_test_split method

"""

# Create training and testing sets
# we will use common split 70-30
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

"""
Now we will use the random forests algorithm. As a first step, we will define a random forests regressor and fit it to the training set. 

The dataset is ready and split into 80% train and 20% test. The features matrix X_train and the array y_train are available. 

"""
# import RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor

# Instantiate rf
rf = RandomForestRegressor(random_state = 2)

"""
====================================================================================================================================
====================================================================================================================================

Without Hyperparameter Tuning

"""

# Fit rf to the training set
rf.fit(X_train, y_train)

"""
we will now evaluate the test set RMSE of the random forests regressor rf that we have trained

"""

# Import mean_squared_error from sklearn.metrics as MSE
from sklearn.metrics import mean_squared_error as MSE

# Compute y_pred
y_pred = rf.predict(X_test)

# Compute mse_dt
rmse_test = MSE(y_test, y_pred) ** (1/2)

# Print rmse_dt
print("Test set RMSE of rf: {:.4f}".format(rmse_test))

"""

Test set RMSE of rf: 0.0147

"""
"""
Now we will determine which features were the most predictive according to the random forests regressor rf that we trained.

For this purpose, we will draw a horizontal barplot of the feature importance as assessed by rf. This can be done easily by plotting capabilities of pandas.

We have created a pandas.Series object called importances containing the feature names as index and their importances as values. In addition,
matplotlib.pyplot is available as plt and pandas as pd.

"""

# Create a pd.Series of features importances
importances = pd.Series(data = rf.feature_importances_,
                        index = X_train.columns)
# Sort importances
importances_sorted = importances.sort_values()

# Draw a horizontal barplot of importances_sorted
importances_sorted.plot(kind = 'barh', color = 'Lightgreen')
plt.title('Features Importances')
plt.show()

"""
====================================================================================================================================
====================================================================================================================================

Hyperparameter Tuning

"""
"""
We will manually set the grid of hyperparameters that will be used to tune rf's hyperparameters and find the optimal regressor.

For this purpose, we will be constructing a grid of hyperparameters and tune the number of estimators such as max_depth, and the maximum number of 
features used when splitting each node and the minimum number of samples (or fraction) per leaf.

1. Set the hyperparameter grid of rf

n_estimators = number of trees in the forest
max_features = max number of features considered for splitting a node
max_depth = max number of levels in each decision tree
min_samples_split = min number of data points placed in a node before the node is split
min_samples_leaf = min number of data points allowed in a leaf node

"""
# Define the dictionary 'params_rf'
params_rf = {
      n_estimators = [100, 350, 500, 1000],
      max_depth = [4, 6, 8],
      max_features = ['Log2', 'auto', 'sqrt'],
      min_samples_leaf = [2, 10, 30]
}

"""
2. Search for the optimal forest

We will perform grid search using 3-fold cross validation to find rf's optimal hyperparameters. To evaluate each model in the grid, we will be
using the negative mean squared error metric.

"""
# Import GridSearchCV
from sklearn.model_selection import GridSearchCV

grid_rf = GridSearchCV(
            estimator = rf,
            param_grid = params_rf,
            scoring = 'neg_mean_squared_error',
            cv = 3,
            verbose = 1,
            n_jobs = -1)
            
"""
3. Searching for the best hyperparameters

"""
# Fit 'grid_rf' to the training set
grid_rf.fit(X_train, y_train)

"""
4. Extracting the best hyperparameters

"""

# Extracting the best hyperparameters from 'grid_rf'

best_hyperparams = grid_rf.best_params_
print('Best hyperparameters:\n', best_hyperparams)

"""
5. Evaluating the best model performance

Now, we will evaluate the test set RMSE of grid_rf's optimal model. For that, we will import the function mean_squared_error from sklearn_metrics
under the alias MSE

"""
# Import mean_squared_error from sklearn.metrics as MSE
from sklearn.metrics import mean_squared_error as MSE

# Extract best model from 'grid_rf'
best_model = grid_rf.best_estimator_

# Predict the test set labels
y_pred = best_model.predict(X_test)

# Evaluate the test set labels
rmse_test = MSE(y_test, y_pred) ** (1/2)

# Print rmse_dt
print("Test set RMSE of grid_rf: {:.4f}".format(rmse_test))

"""
Test set RMSE of rf: 0.0143

"""

# Create a pd.Series of features importances
importances = pd.Series(data = best_model.feature_importances_,
                        index = X_train.columns)
                        
# Sort importances
importances_sorted = importances.sort_values()

# Draw a horizontal barplot of importances_sorted
importances_sorted.plot(kind = 'barh', color = 'Lightgreen')
plt.title('Features Importances')
plt.show()



