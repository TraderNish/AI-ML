# Import the packages

library(tidyverse)
library(caret)
library(dplyr)
library(e1071)
library(psycho)

getwd()

setwd("C:/Users/Nishant/Desktop/S_Score And S_Delta")
df = read.csv("filename.csv")

# Using the dplyr package, let's take a subset of df1:

df2 = df1 %>%
      select(col1, col2, col3)
      
df3 = df1 %>%
      select(class)
      
# preprocess the data by normalizing it
# Scale: The scale transform calculates the standard deviation for an attribute and divides each value by that standard deviation.
# Center: The center transform calculates the mean for an attribute and subtracts it from each value.

# Normalize: Combining the scale and center transform will standardize your data.
# Attributes will have a mean value of 0 and a standard deviation of 1.

# Build your own normalize() function
normalize = function(x) {
  num = x - min(x)
  denom = max(x) - min(x)
  return (num/ denom)
}

# Normalize the data
df4 = as.data.frame(lapply(df2[1:10], normalize))

# Combine normalize dataset with class
df = cbind(df4, df3)

# Summarize the data
summary(df)

# Determine the number of rows for training
nrow(df) * 0.80

# Create a random sample of row IDs 
set.seed(123)
sample_rows = sample(nrow(df), nrow(df) * 0.75)

# Create the training dataset
train = df[sample_rows, ]

# Exclude the training indices to create the test set
test = df[-sample_rows, ]

# Examine the dataset to identify potential independent variable 
str(train)

# Explore the dependent variable 
table(train$class)

# ===========================================================================================================================
# Building Classification Trees Without Prunning
# ===========================================================================================================================

# Load the rpart package
library(rpart)

# Grow a tree using all of the available applicant data
model = rpart(class ~ ., data = train, method = "class", control = rpart.control(cp = 0))

# Make predictions on the test dataset
test$pred = predict(model, test, type = "class")

# Examine the confusion matrix 
table(test$class, test$pred)

# Compute the accuracy on the test dataset
mean(test$class == test$pred)

# Accuracy is 0.5472482

# ===========================================================================================================================
# Preventing Overgrown Classification Trees -- Pre Prunning and Post Prunning
# ===========================================================================================================================

# Grow a tree using all of the available applicant data with maxdepth of 6
model3 = rpart(class ~ ., data = train, method = "class", control = rpart.control(cp = 0, maxdepth = 6))

# Make predictions on the test dataset
test$pred = predict(model3, test, type = "class")


# Compute the accuracy on the test dataset
mean(test$class == test$pred)

# Accuracy is 0.62866

# ===========================================================================================================================
# Swap maxdepth for a minimum split of 500
# ===========================================================================================================================

# Grow a tree using all of the available applicant data with minimum split of 500
model4 = rpart(class ~ ., data = train, method = "class", control = rpart.control(cp = 0, minsplit = 500))

# Make predictions on the test dataset
test$pred = predict(model4, test, type = "class")


# Compute the accuracy on the test dataset
mean(test$class == test$pred)

# Accuracy is 0.6274

# ===========================================================================================================================
# By using post prunning, you can intentionally grow a large and complex tree then prune it to be smaller and more efficient later on
# ===========================================================================================================================

# Grow an overly complex tree
model5 = rpart(class ~ ., data = train, method = "class", control = rpart.control(cp = 0))

# Examine the complexity plot
plotcp(model5)

# Prune the tree
model_pruned = prune(model5, cp = 0.001)

# Make predictions on the test dataset
test$pred = predict(model_pruned, test, type = "class")


# Compute the accuracy on the test dataset
mean(test$class == test$pred)

# Accuracy is 0.6307

# ===========================================================================================================================
# Build pre and post prunning and visualizing classification trees
# ===========================================================================================================================

# Grow an overly complex tree
model7 = rpart(class ~ ., data = train, method = "class", control = rpart.control(cp = 0.001, maxdepth = 6, minsplit = 250))

# Make predictions on the test dataset
test$pred = predict(model7, test, type = "class")


# Compute the accuracy on the test dataset
mean(test$class == test$pred)

# Examine the model object 
model7

# Load the rpart.plot package
library(rpart.plot)

# Plot the loan_model with default settings
rpart.plt(model7)

# Plot the loan_model with customized settings
rpart.plot(model7, type = 3, box.palette = c("red", "green"), fallen.leaves = TRUE)


# ===========================================================================================================================
# Build a random forest model
# ===========================================================================================================================

# Load the randomForest package
library(randomForest)

model8 = randomForest(class ~ ., data = train, ntree = 500)

# Make predictions on the test dataset
test$pred = predict(model8, test)


# Compute the accuracy on the test dataset
mean(test$class == test$pred)
