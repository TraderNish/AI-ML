
# Import the packages

library(tidyverse)
library(caret)
library(dplyr)
library(e1071)
library(psycho)

getwd()

setwd("C:/Users/Nishant/Desktop/S_Score And S_Delta")
df = read.csv("filename.csv")

# Using the dplyr package, let's take a subset of df1:

df2 = df1 %>%
      select(col1, col2, col3)
      
df3 = df1 %>%
      select(class)
      
# ==================================================================================================================================
# Preprocess the data by normalization
# ==================================================================================================================================

# preprocess the data by normalizing it
# Scale: The scale transform calculates the standard deviation for an attribute and divides each value by that standard deviation.
# Center: The center transform calculates the mean for an attribute and subtracts it from each value.

# Normalize: Combining the scale and center transform will standardize your data.
# Attributes will have a mean value of 0 and a standard deviation of 1.

# Build your own normalize() function
normalize = function(x) {
  num = x - min(x)
  denom = max(x) - min(x)
  return (num/ denom)
}

# Normalize the data
df4 = as.data.frame(lapply(df2[1:10], normalize))

# Combine normalize dataset with class
df = cbind(df4, df3)

# Summarize the data
summary(df)

# ==================================================================================================================================
# Splittingt the data into Train and Test
# ==================================================================================================================================

# Total number of rows in the data frame
n = nrow(df)

# Number of rows for the training set (80% of the dataset)
n_train = round(n * 0.80)

# One way you can take a train test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures 
# that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally
# been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this 
# like shuffling a brand new deck of playing cards before dealing hands.

# First, you set a random seed so that your work is reproducible and you get the same random split each time you run your script
# Next, you use the sample() function to shuffle the row indices of the dataset. You can later use these indices to reorder the dataset.

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices = sample(1:n, n_train)

# Subset the data frame to training indieces only
train = df[train_indices, ]

# Exclude the training indices to create the test set
test = df[-train_indices, ]

# Examine the dataset to identify potential independent variables 
str(train)

# Explore the dependent variable
table(train$class)

# ===========================================================================================================================
# ===========================================================================================================================
# Train a classification tree model
# ===========================================================================================================================
# ===========================================================================================================================

# Load the rpart package
library(rpart)

# Grow a tree using all of the available applicant data
LR_model = rpart(class ~ ., data = train, method = "class")

# look at the model output
print(LR_model)

# class in test set should be factors
test$class = as.factor(test$class)

# Generate predicted classes using the model object
LR_Prediction = predict(object = LR_Model, newdata = test, type = "class")

# Calculate the confusion matrix for the test set
confusionMatrix(data = LR_prediction, reference = test$class)

# ===========================================================================================================================
# Compare models with a different splitting criterion
# ===========================================================================================================================

# Train two models that use a different splitting criterion and use the validation set to choose a best model from this group. 
# To do this, use the parms argument of the rpart() function. This argument takes a named list that contains values of different
# parameters you can use to change how the model is trained. Set the parameters split to control the splitting criterion.

# Train a gini-based model 
LR_model_gini = rpart(class ~ ., data = train, method = "class", parms = list(split = "gini"))

# Train an information-based model 
LR_model_info = rpart(class ~ ., data = train, method = "class", parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 = predict(object = LR_Model_gini, newdata = test, type = "class")

# Generate predictions on the validation set using the information model
pred2 = predict(object = LR_Model_info, newdata = test, type = "class")

# Classification error is the fraction of incorrectly classified instances.
# Compute and compare the test set classification error of the two models by using ce() function.
# Compare classification error

library(ModelMetrics)

ce(actual = test$class,
     predicted = pred1)


ce(actual = test$class,
     predicted = pred2)
     
# ===========================================================================================================================
# ===========================================================================================================================
# Train a begged tree model
# ===========================================================================================================================
# ===========================================================================================================================
























