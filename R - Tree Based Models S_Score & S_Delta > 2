
# Import the packages

library(tidyverse)
library(caret)
library(dplyr)
library(e1071)
library(psycho)

getwd()

setwd("C:/Users/Nishant/Desktop/S_Score And S_Delta")
df = read.csv("filename.csv")

# Using the dplyr package, let's take a subset of df1:

df2 = df1 %>%
      select(col1, col2, col3)
      
df3 = df1 %>%
      select(class)
      
# ==================================================================================================================================
# Preprocess the data by normalization
# ==================================================================================================================================

# preprocess the data by normalizing it
# Scale: The scale transform calculates the standard deviation for an attribute and divides each value by that standard deviation.
# Center: The center transform calculates the mean for an attribute and subtracts it from each value.

# Normalize: Combining the scale and center transform will standardize your data.
# Attributes will have a mean value of 0 and a standard deviation of 1.

# Build your own normalize() function
normalize = function(x) {
  num = x - min(x)
  denom = max(x) - min(x)
  return (num/ denom)
}

# Normalize the data
df4 = as.data.frame(lapply(df2[1:10], normalize))

# Combine normalize dataset with class
df = cbind(df4, df3)

# Summarize the data
summary(df)

# ==================================================================================================================================
# Splittingt the data into Train and Test
# ==================================================================================================================================

# Total number of rows in the data frame
n = nrow(df)

# Number of rows for the training set (80% of the dataset)
n_train = round(n * 0.80)

# One way you can take a train test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures 
# that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally
# been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this 
# like shuffling a brand new deck of playing cards before dealing hands.

# First, you set a random seed so that your work is reproducible and you get the same random split each time you run your script
# Next, you use the sample() function to shuffle the row indices of the dataset. You can later use these indices to reorder the dataset.

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices = sample(1:n, n_train)

# Subset the data frame to training indieces only
train = df[train_indices, ]

# Exclude the training indices to create the test set
test = df[-train_indices, ]

# Examine the dataset to identify potential independent variables 
str(train)

# Explore the dependent variable
table(train$class)

# ===========================================================================================================================
# ===========================================================================================================================
# Train a classification tree model
# ===========================================================================================================================
# ===========================================================================================================================

# Load the rpart package
library(rpart)

# Grow a tree using all of the available applicant data
LR_model = rpart(class ~ ., data = train, method = "class")

# look at the model output
print(LR_model)

# class in test set should be factors
test$class = as.factor(test$class)

# Generate predicted classes using the model object
LR_Prediction = predict(object = LR_Model, newdata = test, type = "class")

# Calculate the confusion matrix for the test set
confusionMatrix(data = LR_prediction, reference = test$class)

# ===========================================================================================================================
# Compare models with a different splitting criterion
# ===========================================================================================================================

# Train two models that use a different splitting criterion and use the validation set to choose a best model from this group. 
# To do this, use the parms argument of the rpart() function. This argument takes a named list that contains values of different
# parameters you can use to change how the model is trained. Set the parameters split to control the splitting criterion.

# Train a gini-based model 
LR_model_gini = rpart(class ~ ., data = train, method = "class", parms = list(split = "gini"))

# Train an information-based model 
LR_model_info = rpart(class ~ ., data = train, method = "class", parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 = predict(object = LR_Model_gini, newdata = test, type = "class")

# Generate predictions on the validation set using the information model
pred2 = predict(object = LR_Model_info, newdata = test, type = "class")

# Classification error is the fraction of incorrectly classified instances.
# Compute and compare the test set classification error of the two models by using ce() function.
# Compare classification error

library(ModelMetrics)

ce(actual = test$class,
     predicted = pred1)


ce(actual = test$class,
     predicted = pred2)
     
# ===========================================================================================================================
# ===========================================================================================================================
# Train a begged tree model
# ===========================================================================================================================
# ===========================================================================================================================

# bagging () function from the ipred package. The number of bagged trees can be specified using the nbagg parameter. 
# We can also use the default 25

library(ipred)

# bagging is a randomized model so let's set a seed(123) for reproducibility
set.seed(123)

# Train a bagged model
# If we want to estimate the model's accuracy using the "out-of-bag" samples we can set the coob parameter to TRUE.
# The OOB samples are the training observations that were not selected into the bootstrapeed sample (used in the training)

# Since these observations were not used in training, we can ue them instead to evaluate the accuracy of the model 
# done automatically inside the bagging () function

# class in test set should be factors
test$class = as.factor(test$class)

# Bagging model
model_bagging = bagging(formula = class ~ ., 
                  data = train, 
                  coob = TRUE)

# look at the model output
print(model_bagging)

# out-of-bag estimates of misclassification error: 0.4242

# Predict and Confusion Matrix
# Generate predicted classes using the model object
classII_prediction = predict(object = model_bagging, 
                              newdata = test, 
                              type = "class") # return classification labels

# look at the model output
print(classII_prediction)

# class in test set should be factors
test$class = as.factor(test$class)

# Calculate the confusion matrix for the test set
confusionMatrix(data = classII_prediction, reference = test$class)

# Generate prediction on the test set
pred = predict(object = model_bagging, 
                newdata = test, 
                type = "prob")
                
 # pred is a matrix
 class(pred)
 
 # Look at the pred format
 head(pred)
 
 # Compute the AUC (acutal must be a binary (or 1/0 numeric) vector)
 auc(actual = ifelse(test$class == "1", 1, 0)
                  predicted = pred[, "1"])
                  
# AUC = 0.602773

# ===========================================================================================================================
# Cross validate a bagged tree model in caret
# ===========================================================================================================================
                  
# Use caret::train() with the "treebag" method to train a model and evaluate the model using the cross-validated AUC. The caret package
# allows the user to easily cross-validate any model across any relevant performance metric. 

# In this case, we will use 5-fold cross validation and evaluate cross-validated 
# AUC (Area under the ROC Curve)
# Specify the training configuration

ctrl = trainControl(method = "cv" # Cross-validation
                    number = 5 # 5 folds
                    classProbs = TRUE # For AUC
                    summaryFunction = twoClassSummary) # For AUC
                    
# When caretList() runs a tree-based model, here rpart but also applies to random forests, it converts the factor levels into variables which
# are used to split the tree. For these variables, names starting with a number are not allowed nor that they contain spaces. So for each
# of theses variables, you can convert the level names to valid labels with the following code.

train$class[train$class == 1] = "yes"
train$class[train$class == 0] = "no"

test$class[test$class == 1] = "yes"
test$class[test$class == 0] = "no"

# Cross validate the model using "treebag" method
# Track AUC 

set.seed(1)

caret_model_bagging = bagging(formula = class ~ ., 
                  data = train,
                  method = "treebag",
                  metric = "ROC",
                  trControl = ctrl)

# look at the model output
print(caret_model_bagging)

# Inspect the contents of the model list
names(caret_model_bagging)

# Print the CV AUC
caret_model_bagging$results[, "ROC"]

# Generate predicted classes using the model object
pred = predict(object = caret_model_bagging, 
                              newdata = test, 
                              type = "prob") # return classification labels

 # Compute the AUC (acutal must be a binary (or 1/0 numeric) vector)
 auc(actual = ifelse(test$class == "1", 1, 0)
                  predicted = pred[, "1"])
                  
# AUC = 0.596586


# ===========================================================================================================================
# ===========================================================================================================================
# Random Forest Model
# ===========================================================================================================================
# ===========================================================================================================================
   















)
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
 
 
 
 
 























